from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.apache.kafka.operators.consume import ConsumeFromTopicOperator
from airflow.operators.python import PythonOperator
import json
import logging

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def process_message(message):
    """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Kafka."""
    logger = logging.getLogger(__name__)
    
    try:
        # Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ»ÑÑ‡ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ
        message_key = message.key().decode('utf-8') if message.key() else None
        message_value = message.value().decode('utf-8')
        event_data = json.loads(message_value)
        
        logger.info(f"ğŸ“¥ ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ. ĞšĞ»ÑÑ‡: {message_key}, Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ: {event_data}")
        
        # Ğ‘Ğ¸Ğ·Ğ½ĞµÑ-Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹
        if event_data['event_type'] == 'user_registration':
            logger.info(f"ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ: {event_data['user_id']} Ñ Ñ‚Ğ°Ñ€Ğ¸Ñ„Ğ¾Ğ¼ {event_data['data']['plan']}")
        elif event_data['event_type'] == 'purchase':
            logger.info(f"ĞŸĞ¾ĞºÑƒĞ¿ĞºĞ° Ğ¾Ñ‚ {event_data['user_id']}: Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚ {event_data['data']['product_id']}, ÑÑƒĞ¼Ğ¼Ğ° {event_data['data']['amount']}")
        
        return event_data
        
    except Exception as e:
        logger.error(f"!!! ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: {e}")
        return None

def save_processed_data(**context):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€)."""
    logger = logging.getLogger(__name__)
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ· XCom
    ti = context['ti']
    messages = ti.xcom_pull(task_ids='consume_from_user_events', key='return_value')
    
    if messages:
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ None (Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸)
        successful_messages = [msg for msg in messages if msg is not None]
        logger.info(f"Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ {len(successful_messages)} Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹")
        
        # Ğ—Ğ´ĞµÑÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ² Ğ‘Ğ”, S3 Ğ¸ Ñ‚.Ğ´.
        for record in successful_messages:
            logger.info(f"Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ: {record['user_id']} - {record['event_type']}")
        
        return f"Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ {len(successful_messages)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹"
    else:
        logger.info("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ")
        return "ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ"

with DAG(
    'kafka_consumer_dag',
    default_args=default_args,
    description='DAG Ğ´Ğ»Ñ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Kafka',
    schedule_interval=timedelta(minutes=35),
    catchup=False,
    tags=['kafka', 'consumer'],
) as dag:

    consume_task = ConsumeFromTopicOperator(
        task_id='consume_from_user_events',
        kafka_config_id='kafka_default',
        topics=['user_events'],
        apply_function=process_message,
        commit_cadence='end_of_batch',
        max_messages=50,
        max_batch_size=10,
        consumer_config={
            'auto.offset.reset': 'earliest',
            'group.id': 'airflow-consumer-group',
            'enable.auto.commit': False,
        },
    )

    save_task = PythonOperator(
        task_id='save_processed_data',
        python_callable=save_processed_data,
        provide_context=True,
    )

    consume_task >> save_task